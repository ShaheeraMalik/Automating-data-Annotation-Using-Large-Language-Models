### **README.md**  

```md
# ğŸ“„ Automating Research Paper Annotation Using Large Language Models (LLMs)  

## ğŸš€ Project Overview  
Manually categorizing research papers is time-consuming and tedious, especially when dealing with large datasets. This project automates the **extraction**, **classification**, and **annotation** of research papers using **Google Gemini API** and **multithreading** for speed optimization.  

### âœ¨ **Key Features**  
âœ… **Scraped PDFs Processing**: Reads research papers in **PDF format** (originally scraped from NeurIPS).  
âœ… **Text Extraction**: Extracts the **title and abstract** from the first page.  
âœ… **Automated Classification**: Uses **Google Gemini API** to classify papers into predefined categories.  
âœ… **Efficient Storage**: Saves results into a structured **CSV file** for easy access.  
âœ… **Multithreading for Speed**: Speeds up processing by handling multiple PDFs in parallel.  

### ğŸ“‚ **Project Structure**  
```
ğŸ“¦ Automating-Data-Annotation
â”œâ”€â”€ ğŸ“‚ data/                     # Folder containing PDF files
â”œâ”€â”€ ğŸ“œ main.py                   # Core script for extraction & classification
â”œâ”€â”€ ğŸ“œ requirements.txt           # Required dependencies
â”œâ”€â”€ ğŸ“œ config.py                  # API key configuration
â”œâ”€â”€ ğŸ“œ README.md                  # Project documentation
â””â”€â”€ ğŸ“œ AnnotatedPapers.csv         # Output file with classified results
```

### ğŸ“¥ **Installation & Setup**  
1ï¸âƒ£ **Clone this repository**  
```bash
git clone https://github.com/yourusername/Automating-Data-Annotation.git
cd Automating-Data-Annotation
```
2ï¸âƒ£ **Create a virtual environment** (Optional but recommended)  
```bash
python -m venv env
source env/bin/activate  # Mac/Linux
env\Scripts\activate     # Windows
```
3ï¸âƒ£ **Install required dependencies**  
```bash
pip install -r requirements.txt
```
4ï¸âƒ£ **Set up API key**  
- Create a file `config.py` and add your **Google Gemini API key**  
```python
API_KEY = "your-google-gemini-api-key"
```

### ğŸ“Œ **Usage**  
Run the script to start processing research papers:  
```bash
python main.py
```
- It will read PDFs from the `data/` folder.  
- Extract **title & abstract** from each paper.  
- Use **LLM classification** to assign a category.  
- Save the output in `AnnotatedPapers.csv`.  

### ğŸ¯ **Categories Used for Classification**  
ğŸ“Œ **Machine Learning**  
ğŸ“Œ **Deep Learning**  
ğŸ“Œ **Optimization**  
ğŸ“Œ **Computer Vision**  
ğŸ“Œ **Natural Language Processing (NLP)**  

### ğŸ›  **Challenges Faced**  
ğŸ”¸ **PDF Formatting Issues**: Different PDFs have varying text structures, making extraction inconsistent.  
ğŸ”¸ **API Rate Limits**: Added **retry logic** with exponential backoff to handle rate limits.  
ğŸ”¸ **Processing Speed**: Used **multithreading** to improve efficiency.  

### ğŸ“– **Learn More**  
ğŸ“¢ **Read the Blog Post**: [Read here](https://medium.com/@shaheeramalik533/automating-data-annotation-using-large-language-models-llms-40649611ed3d)  
ğŸ’¼ **LinkedIn Post**: [Check out my experience]("www.linkedin.com/in/shaheera-malik-35b002318")  

### ğŸ¤ **Contributing**  
Contributions are welcome! Feel free to submit issues or pull requests.  

### ğŸ“œ **License**  
This project is open-source and available under the **MIT License**.  
